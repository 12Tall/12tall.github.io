---
title: æˆ‘ç†è§£çš„ç¥ç»ç½‘ç»œ
date: 2022-10-07 15:26:23
tags:
    - neural network  
    - æˆ‘ç†è§£çš„
    - python  
---


æˆ‘ä¸€ç›´å¸Œæœ›ï¼Œèƒ½ä¸èƒ½æœ‰ä¸€ç§éå¸¸éå¸¸ç®€å•ã€å½¢è±¡çš„æ–¹å¼ï¼Œæ¥å‘äººä»¬è§£é‡Šå¤æ‚çš„ç†è®ºã€‚æœ€å¥½æ˜¯èƒ½ç”¨ç”ŸåŠ¨çš„æ¯”å–»ã€è¿åŒå®ƒçš„å†å²ä¸€èµ·ã€å¨“å¨“é“æ¥ã€‚è¿™ç§æ–¹å¼ä¸€å®šéå¸¸é€‚åˆäººä»¬å¯¹äº‹ç‰©çš„ç†è§£çš„ä¹ æƒ¯å§ã€‚

ä¹Ÿè®¸è¿™æ˜¯ä¸€ä¸ªæ­»èƒ¡åŒï¼Œèµ°åˆ°æŸä¸€æ­¥ä¾¿èµ°ä¸ä¸‹å»äº†ã€‚ä¹Ÿè®¸è¿™ä¹Ÿæ˜¯å‰äººèµ°è¿‡æ— æ•°éçš„é“è·¯ï¼Œåªä¸è¿‡è°ä¹Ÿä¸æ›¾æ³¨æ„åˆ°è·¯è¾¹çš„é‡èŠ±ã€‚æˆ‘æ›´æ„¿æ„è¯´è¿™æ˜¯ä¸€ç§æ€ç»´æ–¹å¼ï¼Œä¸­é—´æºæ‚ç€æˆ‘å¤§å­¦ä»¥æ¥å­¦ä¹ ã€æ€è€ƒçš„ç»“æœã€‚è¾¹èµ°è¾¹çœ‹å§ã€‚

<!-- more -->

## å‡½æ•°ä¸é»‘ç›’å­  
è®©æˆ‘ä»¬å›åˆ°åˆä¸­çš„æ•°å­¦è¯¾å ‚ï¼Œå›å¿†ä¸€ä¸‹ä»€ä¹ˆæ˜¯å‡½æ•°ã€‚å¤§æ¦‚çš„å®šä¹‰æ˜¯ä¸€ç»„æ•°æ®åˆ°å¦ä¸€ç»„æ•°æ®çš„æ˜ å°„ã€‚ä½†æ˜¯ä»¥ä¸€ä¸ªæœªçŸ¥çš„æ¦‚å¿µå»å®šä¹‰å¦ä¸€ä¸ªæœªçŸ¥çš„æ¦‚å¿µæ˜¯ä¸€ä»¶å¾ˆå±é™©çš„äº‹æƒ…ï¼šä»€ä¹ˆæ˜¯æ˜ å°„å‘¢ï¼Ÿ

### å•è¾“å…¥å•è¾“å‡ºå‡½æ•°  
ä¸å¦¨è®©æˆ‘ä»¬å°†å‡½æ•°çœ‹ä½œä¸€ä¸ªé»‘ç›’å­ï¼ŒåŒ…å«ä¸€ä¸ªæ•°æ®å…¥å£å’Œä¸€ä¸ªæ•°æ®å‡ºå£çš„é»‘ç›’å­ã€‚ç›’å­å†…éƒ¨ä¼šè¿›è¡Œä¸€ç³»åˆ—çš„åŠ å·¥å¤„ç†å·¥ä½œï¼Œæ¯”å¦‚ï¼šå°†è¾“å…¥çš„æ•°æ®å€¼å¢åŠ 1ï¼Œå¹¶è¾“å‡ºåˆ°å¤–ç•Œã€‚é‚£æˆ‘ä»¬å°±å¯ä»¥å°†è¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºï¼šå¯¹äºè¾“å…¥`x`ï¼Œç»è¿‡å‡½æ•°çš„å¤„ç†åä¼šè¾“å‡º`y=x+1`ã€‚å¥½å·§ä¸å·§ï¼Œå‡½æ•°çš„è‹±æ–‡åå­—æ˜¯`function`ï¼Œä¹Ÿåˆšå¥½æœ‰â€œåŠŸèƒ½â€çš„æ„æ€ã€‚  
![å•è¾“å…¥å•è¾“å‡ºå‡½æ•°](01_function/1in1out.svg)

### å¤šè¾“å…¥å¤šè¾“å‡ºå‡½æ•°  
æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å¦ä¸€ä¸ªç›’å­ï¼ŒåŒ…å«ä¸¤ä¸ªè¾“å…¥`x,y`å’Œä¸¤ä¸ªè¾“å‡º`z1,z2`ï¼Œå…¶ä¸­`z1=x+y; z2=x-y`ã€‚å½“`y` çš„å€¼è¢«é”å®šä¸º`1` æ—¶ï¼Œ`z1=x+1` é€€åŒ–ä¸ºä¸Šæ–‡æåˆ°çš„[å•è¾“å…¥å•è¾“å‡ºå‡½æ•°](#å•è¾“å…¥å•è¾“å‡ºå‡½æ•°)ã€‚  
![å¤šè¾“å…¥å¤šè¾“å‡ºå‡½æ•°](01_function/2in2out.svg)  
å¤šè¾“å…¥å•è¾“å‡ºçš„å‡½æ•°åœ¨ç”Ÿæ´»ä¸­ä¹Ÿæ¯”è¾ƒå¸¸è§ï¼Œæ¯”å¦‚ï¼šå®‰å…¨è¦æ±‚æ¯”è¾ƒé«˜çš„é—¨ç¦ç³»ç»Ÿï¼Œè¦åŒæ—¶æ’å…¥ä¸¤æŠŠé’¥åŒ™æ‰èƒ½æ‰“å¼€ï¼›è®¡ç®—æœºä¸­çš„è°ƒè‰²æ¿é€šè¿‡è®¾ç½®`R,G,B` çš„å€¼æ¥å”¯ä¸€ç¡®å®šä¸€ç§é¢œè‰²ï¼›å¬åˆ°ä¸‹è¯¾é“ƒå¹¶ä¸”åˆ°é¥­ç‚¹äº†æ‰èƒ½å»é£Ÿå ‚åƒé¥­ã€‚æƒ³ä¸€æƒ³ï¼Œç°å®ç”Ÿæ´»ä¸­å¯ä»¥æ€»ç»“å‡ºè§„å¾‹çš„äº‹æƒ…ï¼ŒåŸºæœ¬éƒ½èƒ½ç”¨ä¸€ä¸ªå¤šè¾“å…¥çš„é»‘ç›’å­æ¨¡å—è¡¨ç¤ºã€‚

ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬è®¾è®¡ç³»ç»Ÿæ—¶ï¼Œè¦æ±‚å°½é‡åšåˆ°æ¨¡å—åŒ–ï¼Œæ–°å¢çš„æ¨¡å—ï¼ˆé»‘ç›’å­ï¼‰å¯¹åŸæœ‰çš„ç³»ç»Ÿä¸è¦æœ‰ä¾µå…¥ï¼ˆå³ä¸è¦å½±å“æœ¬æ¥ç³»ç»Ÿçš„å‡½æ•°å¼ï¼‰ï¼Œè¿™æ ·çš„æ¨¡å—èƒ½å¤Ÿåšåˆ°å³æ’å³ç”¨ã€‚å¯¹äºæ¯ä¸€ä¸ªæ¨¡å—è€Œè¨€ï¼Œéœ€è¦å°½é‡åšåˆ°è¾“å‡ºå¯¹äºè¾“å…¥æ²¡æœ‰å½±å“ï¼Œå°±ç®—æœ‰ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½é€šè¿‡ä¼ é€’å‡½æ•°åŒ–ç®€ï¼ˆæ¦‚å¿µå–è‡ªäºè‡ªåŠ¨æ§åˆ¶åŸç†ï¼‰ï¼Œå°†ç³»ç»Ÿåœ¨æ¦‚å¿µå±‚é¢åŒ–ç®€ä¸ºæ–°çš„ç‹¬ç«‹çš„æ¨¡å—æ„æˆã€‚

## ç»Ÿè®¡ä¸æ‹Ÿåˆ  
è‡ªç„¶ç•Œä¸­ä¸€èˆ¬ä¼šå­˜åœ¨ä¸¤ç§å‡½æ•°ï¼šå·²çŸ¥çš„å’ŒæœªçŸ¥çš„ã€‚åƒç”µå‹-ç”µæµå…³ç³»`U=I*R`ã€ç‰›é¡¿ç¬¬äºŒå®šå¾‹`F=ma`ã€å•ä½åœ†ä¸Šçš„ç‚¹ï¼ˆä¸è§’/å¼§åº¦ç›¸å…³ï¼‰åˆ°`x` è½´çš„æŠ•å½±`y=cos(x)`æ˜¯å·²çŸ¥çš„å‡½æ•°ï¼Œæˆ‘ä»¬èƒ½ç›´æ¥å†™å‡ºå®ƒä»¬çš„å‡½æ•°å¼ï¼›å¦ä¸€ç§å¦‚è‚¡ç¥¨ä»·æ ¼å˜åŠ¨ã€å£°éŸ³çš„æ³¢å½¢ã€æ±½è½¦æ²¹è€—éšé€Ÿåº¦çš„å˜åŒ–ï¼Œä¸æ’é™¤å®ƒä»¬æŒ‰æŸäº›è§„å¾‹å˜åŒ–ï¼Œä½†æ˜¯ç”±äºå½±å“çš„å› ç´ è¿‡å¤šï¼Œæˆ‘ä»¬éš¾ä»¥æ‰¾åˆ°ä¸€ä¸ªç²¾ç¡®çš„å‡½æ•°å¼æ¥è¡¨ç¤ºä»–ä»¬çš„å˜åŒ–ã€‚
å¯¹äºä¸çŸ¥é“ç¡®åˆ‡å‡½æ•°å¼çš„å‡½æ•°ï¼Œå¦‚æœæˆ‘ä»¬æƒ³çŸ¥é“æŸä¸€æ®µè¾“å…¥å¯¹åº”ç€è¾“å‡ºå˜åŒ–çš„è§„å¾‹åˆ™éœ€è¦ç”¨åˆ°ç»Ÿè®¡å­¦çš„æ–¹æ³•äº†ã€‚è¿™é‡Œçš„ç»Ÿè®¡å­¦å¹¶ä¸åŒ…æ‹¬æ¦‚ç‡ï¼Œå› ä¸ºæ¦‚ç‡çš„æ¦‚å¿µè¿˜æ˜¯å¤ªæŠ½è±¡äº†ç‚¹ã€‚

### æ•£ç‚¹å›¾  
å¦‚æœæˆ‘ä»¬æƒ³è¦çœ‹åˆ°æŸæ®µè¾“å…¥å†…ï¼Œè¾“å‡ºçš„å˜åŒ–è¶‹åŠ¿ã€‚æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„è¾“å…¥è¾“å‡ºç‚¹è®°å½•ä¸‹æ¥ï¼Œå¦‚æœè®°å½•çš„ç‚¹è¶³å¤Ÿå¤šã€åˆ†å¸ƒè¶³å¤Ÿå‡åŒ€ï¼Œå°±èƒ½å¾—åˆ°è¾ƒä¸ºç²¾ç¡®çš„å‡½æ•°å›¾åƒã€‚å®é™…ä¸Šè®¡ç®—æœºä½œå›¾ä¹Ÿæ˜¯é€šè¿‡è¿™ä¸ªåŸç†ï¼Œå½“ç¦»æ•£çš„ç‚¹è¶³å¤Ÿå¯†é›†æ—¶ï¼Œå°±å¯ä»¥çœ‹æˆä¸€æ¡è¿ç»­çš„çº¿äº†ã€‚  
![æ•£ç‚¹å›¾](02_statistic/scatter-plot.svg)

### æ’å€¼æ³•  
è‡ªç„¶ç•Œä¸­çš„ä¿¡å·ï¼Œä¸€èˆ¬éƒ½æ˜¯è¿ç»­å˜åŒ–çš„ï¼Œä¸ä¼šçªç„¶äº§ç”Ÿè·³å˜ã€‚è¿™å°±è¯´æ˜ï¼Œå¯¹äºä¸¤æ¬¡é—´éš”æ¯”è¾ƒå°çš„è¾“å…¥ï¼Œå…¶è¾“å‡ºä¸€èˆ¬ä¹Ÿå·®è·è¾ƒå°ã€‚äºæ˜¯ï¼Œå¯¹äºæŸä¸€ä¸ªè¾“å…¥å€¼`x1`ï¼Œå¯ä»¥è¯´å…¶å¯¹åº”çš„è¾“å‡ºå–`y1=(y0+y2)/2`ã€‚è¿™ä¾¿æ˜¯æœ€ç®€å•çš„æ’å€¼æ³•ã€‚ 
![ä¸€å…ƒçº¿æ€§æ’å€¼](02_statistic/linear-interpolation.svg)  
 

### æœ€å°äºŒä¹˜æ³•  
æ’å€¼æ³•å¯ä»¥è¾ƒå¿«åœ°æ±‚å–æŸä¸€ç‚¹çš„å‡½æ•°å€¼ï¼Œä½†æ˜¯å…¶å—é‡‡æ ·ç‚¹çš„å½±å“æ¯”è¾ƒå¤§ï¼Œå¯¹å¤šè¾“å…¥å‡½æ•°çš„æ•ˆæœä¸ç†æƒ³ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªé€šç”¨çš„å‡½æ•°å¼æ¥è¡¨ç¤ºæŸä¸ªæ¨¡å—çš„åŠŸèƒ½ã€‚è¿™æ—¶å€™æˆ‘ä»¬å°±å¯ä»¥æ ¹æ®[æ•£ç‚¹å›¾](#æ•£ç‚¹å›¾)ä¸­çš„å‡½æ•°å›¾åƒï¼Œæ¥å‡è®¾ä¸€ä¸ªè¿‘ä¼¼çš„å‡½æ•°æ–¹ç¨‹`f(x)`ï¼šå‚æ•°çš„ç³»æ•°ä¸å›ºå®šï¼Œé€šè¿‡è®¡ç®—é‡‡æ ·ç‚¹åˆ°`f(x)`çš„ç«–ç›´ï¼ˆæ¯”è¾ƒå¥½ç®—ï¼‰è·ç¦»çš„æœ€å°å€¼ï¼Œæ¥å”¯ä¸€ç¡®å®šä¸€ç»„ç³»æ•°ï¼Œè¿›è€Œå¾—åˆ°ä¸€æ¡ç›¸å¯¹æ¯”è¾ƒç†æƒ³çš„æ›²çº¿æ¥æ‹Ÿåˆæ¨¡å—çš„å‡½æ•°å¼ã€‚è¿™ä¾¿æ˜¯æœ€å°äºŒä¹˜æ³•ã€‚  
![æœ€å°äºŒä¹˜æ³•](02_statistic/least-squares-method.svg)  

ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œé‡‡æ ·ç‚¹åˆ†åˆ«ä¸ºï¼š`(x,y)âˆˆ{(0,1),(2,2.5),(3,4.5),(4,5),(5,6)}`ï¼Œæˆ‘ä»¬éœ€è¦å‡è®¾ç›´çº¿`y=kx+b`ã€‚åˆ™æŸä¸€ä¸ªé‡‡æ ·ç‚¹`(xi,yi)`åˆ°è¯¥ç›´çº¿çš„ç«–ç›´è·ç¦»å°±æ˜¯`di=|yi-y(xi)|`ã€‚ä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¹³æ–¹å»æ‰ç»å¯¹å€¼ç¬¦å·ï¼Œæ±‚æ‰€æœ‰`diÂ²`çš„å’Œ`D`çš„æœ€å°å€¼ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥ç”¨`Python`çš„`Sympy`åº“æ¥å¸®åŠ©æ±‚è§£å¾—åˆ°`D`å…³äº`(k,b)`å˜åŒ–çš„å‡½æ•°ï¼›æˆ–è€…ç›´æ¥åˆ©ç”¨`Matlab`æ±‚å‡ºç›¸åº”çš„`(k,b)`ã€‚   

<CodeGroup>
<CodeGroupItem title="Python">

```python
# pip install Sympy
from sympy import *  
k,b = symbols('k b')  # å®šä¹‰ç¬¦å·å˜é‡  
sum = simplify(
    (1.0 - (0.0*k+b))**2 +  # (0,1)
    (2.5 - (2.0*k+b))**2 +  # (2,2.5)
    (4.5 - (3.0*k+b))**2 +  # (3,4.5)
    (5.0 - (4.0*k+b))**2 +  # (4,5)
    (6.0 - (5.0*k+b))**2    # (5,6)
)
print(latex(sum))
# 5.0 b^{2} + 28.0 b k - 38.0 b + 54.0 k^{2} - 137.0 k + 88.5
```    

</CodeGroupItem>

<CodeGroupItem title="Matlab">

```matlab
x=[0,2,3,4,5]
y=[1,2.5,4.5,5,6]
plot(x,y,'o')  %% ä½œæ•£ç‚¹å›¾
k_b=polyfit(x,y,1)
%% k_b = 1.0338    0.9054
```

</CodeGroupItem>
</CodeGroup>

`D`å…³äº`(k,b)`å˜åŒ–çš„å‡½æ•°å›¾åƒå¦‚ä¸‹ï¼š  
![æœ€å°äºŒä¹˜æ³•æœ€å°å€¼](02_statistic/min-value-of-lsm.svg)  

æœ€ç»ˆæ›²çº¿çš„æ–¹ç¨‹ä¸º`y=1.0338x+0.9054`ï¼Œå’Œæƒ³è±¡ä¸­ç¨å¾®æœ‰äº›å‡ºå…¥ï¼Œä½†æ˜¯éšç€é‡‡æ ·ç‚¹çš„å¢å¤šï¼Œè¯¥æ•°å€¼ä¼šè¶Šæ¥è¶Šå‡†ç¡®ã€‚  

### æ¢¯åº¦ä¸‹é™  
é™¤äº†ä½¿ç”¨`Matlab` ç­‰å·¥å…·å¤–ï¼Œè¿˜æœ‰æ²¡æœ‰å…¶ä»–åŠæ³•å¯ä»¥æ±‚`D`çš„æœ€å°å€¼å‘¢ï¼Ÿåˆ†åˆ«æ±‚`D`å…³äº`k,b`çš„åå¯¼æ•°ï¼Œå¯¹äºè¿™ä¸ªä¾‹å­ï¼Œåå¯¼æ•°å…¨éƒ¨ä¸º`0`æ—¶ï¼Œ`D`å–åˆ°æœ€å°å€¼ã€‚  
ç„¶è€Œå¯¹äºè®¡ç®—æœºæ¥è¯´ï¼Œæ±‚æ•°å€¼è§£è¦æ¯”æ±‚è§£æè§£æ›´åŠ ç®€å•ï¼Œæ¯•ç«Ÿè®¡ç®—æœºå°±æ˜¯ç”¨æ¥è®¡ç®—çš„ã€è€Œä¸æ˜¯ç”¨æ¥æ€è€ƒçš„ã€‚  

å¦‚ä½•æ±‚æ•°å€¼è§£ï¼Ÿæˆ‘ä»¬å…ˆå‡è®¾ä¸Šå›¾ä¸­æœ‰ä»»æ„ä¸€ç‚¹`(ki,bi)`ï¼Œåªè¦è®©è¿™ä¸ªç‚¹å§‹ç»ˆæ²¿ç€`D`å‡å°çš„æ–¹å‘ç§»åŠ¨ä¸€ç‚¹ç‚¹`ğ›¿`å°±å¥½äº†ã€‚ä¹Ÿå°±æ˜¯è¯´åº”è¯¥å§‹ç»ˆæ²¿ç€åå¯¼æ•°`<âˆ‚k,âˆ‚b>`å‘é‡ç›¸åçš„æ–¹å‘ç§»åŠ¨ã€‚  
![æ¢¯åº¦ä¸‹é™æ³•](02_statistic/gradient-descent.svg)  
å…¶ä¸­ï¼Œ`<âˆ‚k,âˆ‚b>`è¢«ç§°ä½œæ¢¯åº¦ï¼Œå®Œæ•´çš„å†™æ³•å¦‚ä¸‹ï¼š  
![æ¢¯åº¦](02_statistic/gradient.svg)  
è¿™ç§é€šè¿‡æ¢¯åº¦è¿­ä»£çš„æ–¹æ³•ä¹Ÿå°±å«åš**æ¢¯åº¦ä¸‹é™æ³•**ã€‚ä¹Ÿæ˜¯ç¥ç»ç½‘ç»œä¸­æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ã€‚


## æœ€å°äºŒä¹˜æ³•çš„æ‰©å±•  
ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªå¯¹å•è¾“å…¥å•è¾“å‡ºçš„å‡½æ•°è¿›è¡Œäº†æ‹Ÿåˆï¼Œæœ€ç»ˆå¾—åˆ°äº†ä¸€ç»„ç³»æ•°`(k,b)`æ¥ç¡®å®šäº†ä¸€æ¡ç›´çº¿ã€‚é‚£ä¹ˆå¦‚æœæ˜¯å¤šè¾“å…¥å•è¾“å‡ºã€æˆ–è€…æ˜¯éç›´çº¿çš„å‡½æ•°åº”è¯¥æ€ä¹ˆæ‹Ÿåˆå‘¢ï¼Ÿ  

### é«˜ç»´å½¢å¼
å¯¹äºäºŒç»´å¹³é¢ä¸­çš„ç‚¹æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ç»´çš„çº¿å–æ‹Ÿåˆï¼›å¯¹äºä¸‰ç»´ç©ºé—´ä¸­çš„ç‚¹æˆ‘ä»¬å¯ä»¥ç”¨äºŒç»´çš„å¹³é¢å»æ‹Ÿåˆï¼›å¯¹äº`n` ç»´ç©ºé—´ä¸­çš„ç‚¹ï¼Œæˆ‘ä»¬åˆ™å¯ä»¥ç”¨`n-1` ç»´çš„è¶…å¹³é¢å»æ‹Ÿåˆã€‚å½¢å¼å¦‚ä¸‹ï¼š  
![æœ€å°äºŒä¹˜æ³•çš„é«˜ç»´å½¢å¼](03_lsm/high-dim-lsm.svg)  
å…¶ä¸­`xi`ä½œä¸ºè¾“å…¥é¡¹çš„é›†åˆ ï¼Œä¸€èˆ¬å–`k0*x0=k0`ä½œä¸ºå¸¸æ•°é¡¹ã€‚  

### æ‹Ÿåˆæ›²çº¿  
å³ä½¿æ˜¯é«˜ç»´æœ€å°äºŒä¹˜æ³•ä¹Ÿåªèƒ½æ‹Ÿåˆç›´çº¿ã€å¹³é¢ã€‚å¦‚æœå‡½æ•°çš„è¾“å‡ºå¹¶ä¸æ˜¯åˆ†å¸ƒåœ¨ä¸€æ¡ç›´çº¿ã€å¹³é¢ä¸Šï¼ˆå¦‚ä¸‹å›¾ï¼‰ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ
![å¯¹æ•°å‡½æ•°æ•£ç‚¹å›¾](03_lsm/scatter-ln.svg)  

è¿™é‡Œç”¨`çº¿æ€§`ä¸€è¯æ¥è¡¨ç¤ºç›´çº¿å˜åŒ–çš„æƒ…å½¢ã€‚æˆ‘ä»¬å‡è®¾ï¼ˆæ„é€ ï¼‰å¦ä¸€ä¸ªå‡½æ•°`g(x)=ln(x)`ï¼Œé‚£ä¹ˆ`y~g` å°±æ˜¯çº¿æ€§å˜åŒ–çš„äº†ã€‚æ‰€ä»¥å¯¹äºéçº¿æ€§çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ•°æ®å˜åŒ–çš„è§„å¾‹ï¼Œæ¥æ„é€ ä¸€ä¸ªå‡½æ•°ï¼Œä½¿å…¶éçº¿æ€§åŒ–ã€‚ç›¸å½“äºåŠ äº†ä¸€ä¸ªçº¿æ€§-éçº¿æ€§è½¬æ¢çš„ç¯èŠ‚ï¼Œç„¶åå†è¿›è¡Œæœ€å°äºŒä¹˜æ³•å¤„ç†ã€‚  

è¿™é‡Œçš„éçº¿æ€§å‡½æ•°ä¸€èˆ¬è¢«ç§°ä¸º**æ¿€æ´»å‡½æ•°**ã€‚äººç±»çš„ç¥ç»å…ƒä¸€èˆ¬å½“å¤–ç•Œçš„ä¿¡å·æ‰“åˆ°æŸä¸€ä¸ªå€¼çš„æ—¶å€™æ‰ä¼šè¢«æ¿€æ´»ï¼Œè€Œä¸”è¾“å‡ºä¸è¾“å…¥å¹¶ä¸ä¸€å®šæ˜¯çº¿æ€§å…³ç³»ã€‚è¿™é‡Œçœ‹æ¥çš„è¯å«åšæ¿€æ´»å‡½æ•°è¿˜æ˜¯æŒºå½¢è±¡çš„ã€‚

### å¤åˆå‡½æ•°  
å†è€…ï¼Œå¦‚æœå‡½æ•°å›¾åƒæ¯”è¾ƒå¤æ‚ï¼Œé‚£æˆ‘ä»¬å¯ä»¥é€šè¿‡ç”¨ä¸€ç»„æˆ–è€…å¤šç»„ç®€å•çš„å‡½æ•°å›¾åƒå åŠ æ¥è¿›è¡Œæ‹Ÿåˆã€‚è¿™é‡Œå¾ˆå®¹æ˜“æƒ³åˆ°çº§æ•°çš„æ¦‚å¿µï¼Œäº‹å®ä¸Šé“ç†æ˜¯ä¸€æ ·çš„ã€‚ä¾‹å¦‚ï¼š`y=tanh(2sin(x)+1)-tanh(sin(x+1))+1`çš„å›¾åƒæ˜¯ä¸‹å›¾ä¸­æœ€ç²—çš„è“è‰²æ›²çº¿ï¼Œå®ƒæ˜¯ç”±ä¸€ç³»åˆ—ç®€å•å‡½æ•°ä¾æ¬¡ä¼ é€’ç»„æˆçš„ã€‚  
![å¤åˆå‡½æ•°å›¾åƒ](03_lsm/composite-function.svg)  
è€Œå¯¹åº”çš„å‡½æ•°é—´çš„ä¼ é€’æ–¹å¼å¦‚ä¸‹ï¼š  
![å¤åˆå‡½æ•°åˆ†è§£](03_lsm/composite-function2.svg
)

#### å¤åˆå‡½æ•°æ±‚å¯¼  
å¯¹äºå¤åˆå‡½æ•°çš„æ±‚å¯¼è¿‡ç¨‹ï¼Œæˆ‘ä»¬æœ‰å…¬å¼ï¼š
![å¤åˆå‡½æ•°æ±‚å¯¼](03_lsm/composite-function-gradient.svg
)  
åå¯¼æ•°çš„æ±‚æ³•ä¹Ÿæ˜¯ä¸€æ ·ï¼Œåˆ†åˆ«æ±‚å‡ºæ¯ä¸€æ­¥çš„å¯¼æ•°ï¼Œæœ€åä¹˜èµ·æ¥å°±æ˜¯æœ€ç»ˆçš„å¯¼æ•°ã€‚  

### è¯¯å·®å‡½æ•°  
å‰é¢æˆ‘ä»¬ä½¿ç”¨äº†å¹³å‡å€¼ï¼Œæ•°å€¼è·ç¦»çš„æœ€å°å€¼æ¥è¡¨ç¤ºæ‹Ÿåˆçš„ç¨‹åº¦ã€‚å…¶å®æˆ‘ä»¬è¿˜å¯ä»¥é€‰æ‹©å…¶ä»–çš„å‡½æ•°ä½œä¸ºè¯¯å·®å‡½æ•°ã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç‚¹åˆ°ç›´çº¿çš„è·ç¦»ã€ç‚¹åˆ°å¹³é¢çš„è·ç¦»ç­‰ç­‰ã€‚ä½†æ˜¯åªè¦ä¿è¯ä¸€æ¡ï¼šæœ€ç»ˆçš„è¯¯å·®å‡½æ•°åœ¨å®šä¹‰åŸŸå†…åº”è¯¥æœ‰ä¸”åªæœ‰ä¸€ä¸ªæœ€å°å€¼ã€‚å¦åˆ™åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ—¶å¯èƒ½ä¼šé™·å…¥æŸä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£ã€‚ä¹Ÿå°±æ˜¯è¯´è¯¯å·®å‡½æ•°`D` æœ€å¥½åƒä¸‹å›¾çš„å·¦è¾¹è€Œä¸æ˜¯å³è¾¹ï¼š 
![è¯¯å·®å‡½æ•°](03_lsm/diff-function.svg)


## å‘é‡ä¸çŸ©é˜µ  
æ¥çœ‹ä¸€ä¸ªé¸¡å…”åŒç¬¼é—®é¢˜ï¼šä»Šæœ‰é¸¡å…”åŒç¬¼ï¼Œå…±æœ‰å¤´36ï¼Œè„š48ï¼Œé—®é¸¡ã€å…”å„å‡ ä½•ï¼Ÿæ˜“å¾—å…”12ã€é¸¡24ï¼›å†é—®ï¼šä»Šç¬¼æœ‰ä¿©é¸¡ä»¨å…”ï¼Œé—®æœ‰å¤´ã€è„šå„å‡ ä½•ï¼Ÿ

### ç‰¹å¾ä¸å‘é‡  
åœ¨æ­¤ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«ç”¨ä¸€ç»„æ•°æ®æ¥ä»£è¡¨é¸¡å’Œå…”çš„ç‰¹å¾ï¼š`1ï¼ˆå¤´ï¼‰ï¼Œ2ï¼ˆè„šï¼‰ï¼›1ï¼ˆå¤´ï¼‰ï¼Œ4ï¼ˆè„šï¼‰`ã€‚æˆ‘ä»¬ä¸€èˆ¬ç®¡è¿™æ ·ä¸€ç»„æ•°æ®å«åš**ç‰¹å¾å‘é‡**ã€‚é¡¾åæ€ä¹‰ï¼Œè¡¨ç¤ºç‰¹å¾çš„æœ‰æ–¹å‘çš„é‡ã€‚  
![å‘é‡](04_matrix/vector.svg)  

å¯¹äºä¸¤ä¸ªå‘é‡å¼ æˆçš„ç©ºé—´ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç§°ä¸ºé¢ç§¯ï¼›ä¸‰ç»´çš„å«åšä½“ç§¯ã€‚é¢ç§¯å¯ä»¥è¡¨ç¤ºç©ºé—´çš„å¤§å°ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªå‘é‡å¹³è¡Œçš„ç¨‹åº¦ã€‚å› ä¸º`S=|a||b|sin(Î±)`ï¼Œ`Î±`è¡¨ç¤ºå‘é‡`a,b`ä¹‹é—´çš„å¤¹è§’ã€‚å‘é‡æ­£ç¡®çš„å†™æ³•åº”è¯¥æ˜¯å­—æ¯ä¸Šé¢æœ‰ä¸€ä¸ªç®­å¤´ï¼Œä½†æ˜¯`markdown` å†…ä¼¼ä¹ä¸æ”¯æŒè¿™ç§å†™æ³•ï¼š![å‘é‡çš„å†™æ³•](04_matrix/std-vector.svg)

### çŸ©é˜µä¸è¡Œåˆ—å¼  
å›åˆ°æœ€åˆçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åˆ—ä¸€ä¸ªè¡¨æ ¼A æ¥è¡¨ç¤ºé¸¡å…”ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç»„åˆï¼›ç”¨å¦ä¸€ä¸ªè¡¨æ ¼è¡¨ç¤ºç¬¼çš„ç‰¹å¾å‘é‡`2ï¼ˆé¸¡ï¼‰3ï¼ˆå…”ï¼‰`ã€‚ç„¶åé—®é¢˜å°±å˜æˆäº†ï¼Œæˆ‘ä»¬éœ€è¦ç”¨`å¤´ï¼Œè„š`æ¥è¡¨ç¤ºç¬¼çš„ç‰¹å¾ã€‚    

![çŸ©é˜µä¹˜æ³•](04_matrix/mul.svg)

å…ˆè®¡ç®—å¤´çš„ç‰¹å¾`h=1x2+1x3=5`ï¼Œå†è®¡ç®—è„šçš„ç‰¹å¾`f=2x2+4x3=16`ã€‚å¯ä»¥æ€»ç»“å‡ºï¼ŒA çš„æ¯ä¸€è¡Œä¸B çš„æ¯ä¸€åˆ—çš„å¯¹åº”é¡¹ç›¸ä¹˜ä¹‹å’Œå°±æ˜¯æœ€åçš„ç»“æœï¼Œè®°ä½œçŸ©é˜µçš„ä¹˜æ³•`C=AB`ã€‚    
çœ‹åˆ°è¿™é‡Œæ˜¯ä¸æ˜¯è§‰å¾—ç‰¹ç†Ÿæ‚‰ã€‚çŸ©é˜µä¹˜æ³•é‡Œé¢æ¯ä¸€è¡Œçš„è¿ç®—å’Œå¤šé¡¹å¼çš„è¿ç®—å½¢å¼ä¸€æ¨¡ä¸€æ ·ï¼š![å¤šé¡¹å¼](04_matrix/polynomial.svg)  
æç¤ºï¼šæŠŠ`x` çœ‹ä½œè¡Œã€æŠŠ`k` çœ‹ä½œåˆ—ã€‚

### Numpyä¸­çš„çŸ©é˜µè¿ç®—  
åœ¨`numpy` ä¸­å®šä¹‰äº†ä¸€ç³»åˆ—çš„çŸ©é˜µè¿ç®—å‡½æ•°ï¼Œè¿™é‡Œç®€å•æ•´ç†å‡ ä¸ªå¸¸ç”¨çš„ï¼Œåç»­å†æ…¢æ…¢å¢åŠ ã€‚  


```python
import numpy as np  
x = np.array([1,2,3])  # å®šä¹‰è¡Œå‘é‡
# [1, 2, 3]

x = x.reshape(3, -1)  # é‡æ–°æ’åˆ—å…ƒç´   
# é‡æ–°æ’åˆ—ä¸º3 è¡ŒçŸ©é˜µï¼Œåˆ—æ•°`-1`è¡¨ç¤ºç”±å‡½æ•°è®¡ç®—å†³å®šæœ‰å¤šå°‘åˆ—  

y=0.5
y=x*y  # çŸ©é˜µA*B è¡¨ç¤ºå¯¹åº”å…ƒç´ ç›¸ä¹˜
# y ä¼šè¢«è‡ªåŠ¨æ‰©å±•ä¸ºå°ºå¯¸åŒxï¼Œå…ƒç´ å…¨éƒ¨ä¸º0.5 çš„çŸ©é˜µ  
# æ­¤æ“ä½œæˆä¸ºå¹¿æ’­
# +,-,*,/,power,log,sqrt,log ç­‰è¿ç®—å‡é€‚ç”¨  

z=x.dot(y.T+1)  # çŸ©é˜µä¹˜æ³•  
# å³ä¸Šé¢é¸¡å…”åŒç¬¼é—®é¢˜æ¨å¯¼å‡ºæ¥çš„çŸ©é˜µä¹˜æ³• 
# .T è¡¨ç¤ºçŸ©é˜µçš„è½¬ç½®  
'''z=
[[1.5 2.  2.5]
 [3.  4.  5. ]
 [4.5 6.  7.5]]
'''

print(z.mean(axis=0))  # è¾“å‡ºå¹³å‡å€¼  
# axis è¡¨ç¤ºçŸ©é˜µçš„ç»´åº¦åºå·
# [3. 4. 5.]
print(z.mean(axis=1))  # å¯¹ç…§z çš„å€¼ç†è§£
# [2. 4. 6.]
# ä¸æ˜¯å¤ªå¥½ç†è§£ï¼Œå¤§è‡´è¡¨ç¤ºå¯¹ç¬¬n å±‚çš„å…ƒç´ æ±‚å¹³å‡å€¼

np.zeros((row, col))  # åˆå§‹åŒ–0 çŸ©é˜µ
np.around(z)  # å››èˆäº”å…¥  
z.flatten()  # å¹³å¦åŒ–ï¼ŒæŒ‰æŸç§é¡ºåºè½¬åŒ–ä¸ºè¡Œå‘é‡ï¼Œä¸€èˆ¬æ˜¯æŒ‰C æ•°ç»„çš„æ–¹å¼  
```

## å•ç¥ç»å…ƒç½‘ç»œ  
å…¶å®ç¥ç»ç½‘ç»œè¦åšçš„ï¼Œä¹Ÿåªæ˜¯æ„é€ ä¸€ä¸ªå‡½æ•°ï¼Œå»æ‹Ÿåˆä¸€æ®µæ•°æ®çš„å˜åŒ–æƒ…å†µç½¢äº†ã€‚åªä¸è¿‡æˆ‘ä»¬è®©ç”µè„‘åˆ©ç”¨[æ¢¯åº¦ä¸‹é™](#æ¢¯åº¦ä¸‹é™)çš„åŠæ³•é€æ¸è°ƒæ•´æœ€ä¼˜ç³»æ•°ç»„åˆã€‚ä¸‹é¢æˆ‘ä»¬è¿˜æ˜¯ä»¥[æœ€å°äºŒä¹˜æ³•](#æœ€å°äºŒä¹˜æ³•)ä¸­çš„æ•°æ®ä¸ºä¾‹ï¼Œæ¥çœ‹çœ‹ç¥ç»ç½‘ç»œçš„æ„é€ è¿‡ç¨‹ä¸å·¥ä½œåŸç†ã€‚  

### æ¢¯åº¦ä¸‹é™çš„æœ€å°äºŒä¹˜æ³•  
![æœ€å°äºŒä¹˜æ³•](02_statistic/least-squares-method.svg)  
é¦–å…ˆæå–è®¡ç®—éœ€è¦ç”¨åˆ°çš„ä¿¡æ¯ï¼š  
- é‡‡æ ·æ¬¡æ•°`n`ï¼š5  
- è¾“å…¥ç‰¹å¾å‘é‡çš„ç»´åº¦`m`ï¼š1  
- è¾“å…¥æ•°æ®`X`ï¼š`[0, 2, 3, 4, 5]`  
- è¾“å‡ºæ•°æ®çš„ç»´åº¦ï¼š1  
- è¾“å‡ºæ•°æ®`Y`ï¼š`[1, 2.5, 4.5, 5, 6]`  

ç„¶åé’ˆå¯¹**æŸä¸€æ¬¡é‡‡æ ·æ•°æ®**æ„é€ å‡½æ•°ä¼ é€’çš„è¿‡ç¨‹ï¼Œè¿™é‡Œå› ä¸ºè¾“å‡ºç»“æœä¹Ÿæ˜¯çº¿æ€§çš„ï¼Œæ‰€ä»¥æ¿€æ´»å‡½æ•°å¯ä»¥å–`a(z)=z`ï¼Œå³ä¸åšä»»ä½•å¤„ç†ï¼Œç›¸å½“äºä¸€ä¸ªé€æ˜ç¯èŠ‚ã€‚å…¶å¯¼æ•°å€¼æ’ä¸º`1`    
![æœ€å°äºŒä¹˜æ³•å‡½æ•°ä¼ é€’](05_single_nerual_network/lsm-function-structure.svg)  
è€Œæˆ‘ä»¬çš„æ ·æœ¬æ•°é‡ä¸º5ï¼Œè¿™æ ·æ¯æ¬¡è®¡ç®—ä¼šå¾—åˆ°5 ä¸ªè¯¯å·®å€¼ã€‚ä¸€èˆ¬æˆ‘ä»¬å–å…¶å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆçš„è¯¯å·®ã€‚  

ä¸‹é¢æ˜¯æ˜¯`Python` çš„ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°éšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ ï¼Œè¯¯å·®é€æ¸å‡å°ã€‚  
```python
import numpy as np

# åˆå§‹åŒ–å‚æ•°  
x = np.array([0,2,3,4,5]).reshape(5, -1)
y = np.array([1, 2.5, 4.5, 5, 6]).reshape(5, -1)
k,b=(0,0)
delta=0.01  # æ­¥é•¿ä¸å®œè¿‡å¤§ï¼Œå¦åˆ™é€’å½’ä¸‹é™çš„è¿‡ç¨‹ä¼šäº§ç”Ÿéœ‡è¡

# çº¿æ€§è¿‡ç¨‹ï¼šz=kx+b
def Z(x):      
    return k*x+b
def dz_dk(x):  # zå¯¹k çš„åå¯¼
    return x
def dz_db(x):  # zå¯¹b çš„åå¯¼
    return 1

# æ¿€æ´»å‡½æ•°ï¼ša=z ç”±æ•£ç‚¹å›¾å¯è®¾æ¿€æ´»å‡½æ•°æ˜¯çº¿æ€§çš„
def Act(z):    
    return z
def dAct(z):   # æ¿€æ´»å‡½æ•°çš„åå¾®åˆ†
    return 1

# è¯¯å·®å‡½æ•°ï¼š(y-a)^2ï¼Œæœ€å°äºŒä¹˜æ³•
def Diff(a, y):    
    return (y-a)*(y-a)
def dDiff(a, y):
    return -2*(y-a)

# åå¤è¿­ä»£æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
for i in range(10000):
    z = Z(x)       # æ±‚å–æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„è®¡ç®—ç»“æœ
    a = Act(z)
    d = Diff(a, y)
    # æ±‚å–è¯¯å·®å‡½æ•°å¯¹k,b çš„åå¾®åˆ†ï¼Œæ¯æ¬¡é‡‡æ ·éƒ½æ˜¯ä¸åŒçš„
    dd_dk = dDiff(a, y)*dAct(z)*dz_dk(x)  
    dd_db = dDiff(a, y)*dAct(z)*dz_db(x)
    # æ¢¯åº¦ä¸‹é™ï¼šæ¯æ¬¡è¿­ä»£éƒ½ä¼šè®¡ç®—å››æ¬¡é‡‡æ ·ä¸‹çš„å¹³å‡å€¼
    k = k-delta*dd_dk.mean(axis=0)
    b = b-delta*dd_db.mean(axis=0)
    # è¾“å‡ºç¬¬i æ¬¡è¿­ä»£çš„ç»“æœ

print(k, b, d.mean(axis=0))
# [1.03378378] [0.90540541] [0.09662162]
```

### å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«  
æ­¤é¢˜ä¸ºå´æ©è¾¾æ·±åº¦å­¦ä¹ çš„è¯¾åç¼–ç¨‹é¢˜[Logistic Regression with a Neural Network mindset](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)ã€‚æ ¹æ®å›¾ç‰‡ä¸­çš„åƒç´ ä¿¡æ¯æ¥åˆ¤æ–­å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«ã€‚  

é¦–å…ˆè¿˜æ˜¯æ¢³ç†å…³é”®ä¿¡æ¯ï¼š  
- ä»å›¾ç‰‡ä¸­è¯»å–æ•°æ®ï¼Œæ¯å¼ å›¾ç‰‡åŒ…å«`3*64*64` å­—èŠ‚æ•°æ®ï¼Œä¹Ÿå°±æ˜¯æœ‰`3*64*64`ä¸ªç»´åº¦  
- ç”¨æ¥è®­ç»ƒçš„å›¾ç‰‡æœ‰`209`å¼ ï¼›æµ‹è¯•ç”¨çš„`50`å¼   
- è¾“å‡ºæ˜¯çœŸå‡ï¼Œæ˜¯éçº¿æ€§çš„ï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªéçº¿æ€§è¯å‡½æ•°ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨`sigmoid`å‡½æ•°ã€‚ä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œä¸€èˆ¬æˆ‘ä»¬é‡‡ç”¨`D(a,y)=-(yln(a)-(1-y)ln(1-a))`ä½œä¸ºè¯¯å·®å‡½æ•°ã€‚  

![Sigmoidå‡½æ•°å›¾åƒ](05_single_nerual_network/sigmoid.svg)

ä¸‹é¢æ˜¯å‡½æ•°ä¼ é€’çš„ç¤ºæ„å›¾ï¼Œä»…ä»¥äºŒç»´è¾“å…¥ç¤ºæ„ï¼Œå®é™…è¾“å…¥åº”è¯¥æ˜¯`3*64*64`ç»´ï¼š  
![å‡½æ•°ç»“æ„](05_single_nerual_network/function-structure.svg)

é™¤äº†ä¿®æ”¹æ•°æ®åˆå§‹åŒ–ã€æ¿€æ´»å‡½æ•°ä¸è¯¯å·®å‡½æ•°å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å‡ ä¹ç…§æ¬[æ¢¯åº¦ä¸‹é™çš„æœ€å°äºŒä¹˜æ³•](#æ¢¯åº¦ä¸‹é™çš„æœ€å°äºŒä¹˜æ³•)é‡Œé¢çš„ä»£ç ã€‚  
```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from lr_utils import load_dataset

# æ•°æ®åŠ è½½è¿‡ç¨‹é¢˜ç›®å·²ç»ç»™å‡º 
delta=0.1  
dim = 3*64*64  # å›¾ç‰‡æ•°æ®ç»´åº¦  
k = np.zeros((dim, 1))
b = np.zeros((1, 1))
len_train = 209
len_test = 50

# å°†è¾“å…¥æ•°æ®å½’ä¸€åŒ–ï¼Œæ›´æœ‰åˆ©äºé¿å…æŒ‡æ•°è¿ç®—ä¸­çš„æ•°æ®æº¢å‡º
x = (train_set_x_orig.reshape(-1, dim))/255-0.5
y = train_set_y.reshape(-1, 1)
# æµ‹è¯•æ•°æ®  
tx = (test_set_x_orig.reshape(-1, dim))/255-0.5
ty = test_set_y.reshape(-1, 1)

# çº¿æ€§è¿‡ç¨‹ï¼šz=kx+b
def Z(x):      
    return k*x+b
def dz_dk(x):  # zå¯¹k çš„åå¯¼
    return x
def dz_db(x):  # zå¯¹b çš„åå¯¼
    return 1

# æ¿€æ´»å‡½æ•°è¿›è¡Œéçº¿æ€§åŒ–
def Act(z):
    return 1/(1+np.exp(-z))
def dAct(z):  # æ¿€æ´»å‡½æ•°æ±‚å¯¼
    a = Act(z)
    return a*(1-a)

# å¯¹æ•°è¯¯å·®å‡½æ•°åŠå…¶å¯¼æ•°
def Diff(a, y):
    return -(y*np.log(a)+(1-y)*np.log(1-a))
def dDiff(a, y):
    return (a-y)/a/(1-a)

# åå¤è¿­ä»£æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
for i in range(2000):
    z = Z(x)
    a = Act(z)
    d = Diff(a, y)

    # è¿™é‡Œé™¤ä»¥ä¸€ä¸ªè¾ƒå¤§çš„æ•°ï¼Œä¹Ÿæ˜¯ä¸ºäº†é¿å…æŒ‡æ•°è¿ç®—æ—¶çš„æ•°æ®çˆ†ç‚¸
    dk = np.dot(dZ_dk(x).T, dDiff(a, y)*dAct(z))/len_train
    db = (dDiff(a, y)*dAct(z)).sum()/len_train
    k = k-delta*dk
    b = b-delta*db

# ä»¥ä¸‹ä»£ç ä»…ä½œç»“æœåˆ†æåªç”¨ï¼Œå¹¶ä¸å½±å“ç¥ç»ç½‘ç»œæœ¬èº«çš„è®¡ç®—
def Res(x, y):
    z = Z(x)
    a = Act(z)
    diff = np.around(np.abs(a-y))
    # è¾“å‡ºå‡†ç¡®ç‡
    print(1-diff.sum()/len(x))
    return diff

diff = Res(tx, ty).flatten()
print(ty)

# ç»˜åˆ¶è¯†åˆ«é”™è¯¯çš„å›¾åƒ
plt.figure()
for i in range(len(diff)):
    if diff[i] > 0.5:
        plt.subplot(5,10,i+1)
        plt.imshow(test_set_x_orig[i].reshape((64, 64, 3)))

plt.show()
```
é€šè¿‡ä»£ç ç»“æœå¯ä»¥çœ‹å‡ºæ¥ï¼Œæ¨¡å‹å¯¹è®­ç»ƒé›†çš„æ‹Ÿåˆéå¸¸å¥½ï¼Œè€Œå¯¹äºæµ‹è¯•é›†çš„å‡†ç¡®åº¦æœ€é«˜åªæœ‰68%ï¼Œå¹¶ä¸”åƒæ˜¯å¯¹è®­ç»ƒé›†è¿‡åº¦æ‹Ÿåˆï¼Œä»¥è‡³äºåªèƒ½è¯†åˆ«èŒƒå›´æ¯”è¾ƒçª„çš„åŒºåŸŸï¼Œè¿™å¯èƒ½å·²ç»æ˜¯å•ä¸ªç¥ç»å…ƒèƒ½è¾¾åˆ°çš„æé™äº†ã€‚

### å®ç”¨å·¥å…·  
åœ¨ä¸Šæ–‡ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹æŸäº›å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡`Î´`ï¼Œæ¥ç”¨æ›´å°‘çš„è®­ç»ƒæ¬¡æ•°å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚é‚£ä¹ˆæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡è®©æœºå™¨åœ¨ä¸€ä¸ªèŒƒå›´å†…è°ƒæ•´æŸå‡ ä¸ªå‚æ•°ã€è®°å½•å¹¶ç»˜åˆ¶è¯¯å·®éšè¿­ä»£æ¬¡æ•°çš„å…³ç³»å›¾ï¼Œæ¥å¸®åŠ©æˆ‘ä»¬é€‰æ‹©æ›´åˆé€‚çš„æ¨¡å‹å‚æ•°ã€‚  
```python
# 1. æ˜¾ç¤ºå›¾å½¢
import matplotlib.pyplot as plt
# 1.1 æ˜¾ç¤ºå•å¼ å›¾ç‰‡
plt.imshow(img.reshape((w, h, 3))) # RGB å›¾åƒçš„å®½å’Œé«˜
# 1.2 æ˜¾ç¤ºå¤šå¼ å›¾ç‰‡
# ç»˜åˆ¶è¯†åˆ«é”™è¯¯çš„å›¾åƒ
plt.figure()
for img in range(imgData):
        plt.subplot(n,m,i+1)  # æ˜¾ç¤ºåœ¨nxm å¼ å›¾ç‰‡çš„ç¬¬i ä¸ªä½ç½®
        plt.imshow(img.reshape((w, h, 3))) # RGB å›¾åƒçš„å®½å’Œé«˜
plt.show()

# 2. ç»˜åˆ¶æ›²çº¿å›¾
data = np.squeeze(orgData)  # åˆ é™¤æ— å…³çš„ç»´åº¦
plt.plot(data)              # ç»˜å›¾
plt.ylabel('y')             # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
plt.xlabel('x')
plt.title("title")
plt.show()

# 3. è¯»å–å›¾ç‰‡ä¿¡æ¯
from PIL import Image
image = Image.open(my_file).convert('RGB')
image = np.array(image)
```
Python ä¸éš¾å­¦ï¼Œéš¾çš„æ˜¯æŒæ¡ç›¸å…³çš„åº“å‡½æ•°ä¸åŸºç¡€çš„ç†è®ºçŸ¥è¯†ã€‚çŸ¥å…¶æ‰€ä»¥ç„¶åæ‰èƒ½ç”¨å¥½Pythonï¼Œæ‰€ä»¥Python æ˜¯ä¸€ä¸ªè®©äººè®¨åŒçš„å¼ºå¤§çš„å·¥å…·ã€‚  

## æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ  
æ­£å¦‚[å¤åˆå‡½æ•°](#å¤åˆå‡½æ•°)ä¸­æ‰€å±•ç¤ºçš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»„åˆä¸åŒçš„çº¿æ€§ã€éçº¿æ€§å‡½æ•°ä¸€æ­¥æ­¥å¾—åˆ°å¤æ‚çš„å‡½æ•°å›¾åƒã€‚è€Œå¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œæ¯ä¸€ä¸ªçº¿æ€§+éçº¿æ€§çš„å‡½æ•°å¯¹å°±ç›¸å½“äºä¸€ä¸ªç¥ç»å…ƒèŠ‚ç‚¹ã€‚é€šè¿‡åå¤è¿­ä»£æ¥ä¿®æ”¹æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„ç³»æ•°ï¼Œå°±èƒ½å¾—åˆ°ä¸€ç»„æœ€ä¼˜ç³»æ•°ï¼Œè®©æˆ‘ä»¬çš„å‡½æ•°æ¨¡å‹ä½•ä»¥å°½å¯èƒ½ç²¾ç¡®åœ°æ‹Ÿåˆå®é™…çš„é‡‡æ ·ä¿¡æ¯ã€‚  

å¯¹äºåˆšå…¥é—¨çš„æˆ‘æ¥è¯´ï¼Œæœ€éš¾çš„åœ°æ–¹é™¤äº†Python ä¹‹å¤–ï¼Œè¿˜æœ‰å°±æ˜¯æ‰¾ä¸åˆ°ç»ƒæ‰‹çš„ä¾‹å­ä¸æ•°æ®ã€‚äºæ˜¯åªèƒ½ç”¨å´æ©è¾¾è€å¸ˆçš„ä¾‹é¢˜[Planar data classification with one hidden layer](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)åšç¬”è®°äº†ã€‚ä¸ºäº†å‡å°‘ä¸ç°æœ‰å¸¸ç”¨ç¬¦å·çš„å†²çªï¼Œæˆ‘ä»¬å°†åœ¨ä»¥åæŠŠæ–œç‡kï¼Œå†™ä½œæƒé‡wã€‚

### çŸ©é˜µçš„å¾®åˆ†  
åœ¨æ­£å¼å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å†æœ€åå­¦ä¹ ä¸€éæ±‚å¾®åˆ†ï¼ˆå¯¼æ•°ï¼‰çš„è¿‡ç¨‹ã€‚å› ä¸ºå¯¹äºè®¡ç®—æœºæ¥è¯´ï¼Œæ¯ä¸€çº§å‡½æ•°éƒ½æ˜¯æœ‰æ•°æ®è¾“å…¥çš„ï¼Œæ‰€ä»¥åœ¨å‡½æ•°æ­£å‘è¿‡ç¨‹ä¸­ï¼Œå°±èƒ½æŠŠå¯¹åº”è¾“å…¥è¾“å‡ºçš„åå¾®åˆ†æ±‚è§£å‡ºæ¥ã€‚ç„¶åå†åˆ©ç”¨å¤åˆå‡½æ•°çš„å¾®åˆ†æ³•åˆ™ï¼Œæ¥æ„é€ æœ€ç»ˆçš„ç»“æœå³å¯ã€‚  
- å¤åˆå‡½æ•°æ±‚å¯¼ï¼š![å¤åˆå‡½æ•°æ±‚å¯¼](06_deep_learning/composite-func.svg)  
- çŸ©é˜µçš„æ±‚å¯¼ï¼šè§ä¸‹å›¾  
![çŸ©é˜µçš„å¾®åˆ†](06_deep_learning/matrix-dimension.svg)  

### é€»è¾‘å›å½’  
ç»™å®šçš„ä¸€ç»„æ•°æ®åŒ…å«ä¸‰ä¸ªç»´åº¦`(x,y,z)`ï¼Œå…¶ä¸­`z`ä½œä¸ºè¾“å‡ºã€‚å„ä¸ªæ•°æ®ç‚¹ä¹‹é—´é”™ç»¼åˆ†å¸ƒï¼Œæ²¡æœ‰æ˜æ˜¾çš„åˆ†ç•Œçº¿ã€‚æ‰€ä»¥é€šè¿‡å¹³é¢ã€æˆ–è€…ç®€å•çš„æ›²é¢æ— æ³•æ‹Ÿåˆè¿™ä¹ˆå¤æ‚çš„å…³ç³»ã€‚æ‰€ä»¥æˆ‘ä»¬è€ƒè™‘æ·»åŠ ä¸€å±‚ç¥ç»å…ƒï¼Œçœ‹èƒ½å¦å¾—åˆ°æ¯”è¾ƒå‡†ç¡®çš„æ¨¡å‹ã€‚  
![å¹³é¢æ•°æ®åˆ†å¸ƒ](06_deep_learning/planar-scatter.svg)  

æˆ‘ä»¬å¯ä»¥å¾—åˆ°è¾“å…¥è¾“å‡ºæ•°æ®å¤§å°åˆ†åˆ«æ˜¯`X=2x400`ã€`Y=1x400`ã€‚åœ¨æ·±å…¥ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨`sklearn`å†…ç½®çš„é€»è¾‘å›å½’æ¥å°è¯•æ‹Ÿåˆä»¥ä¸‹ã€‚é€»è¾‘å›å½’ä¹Ÿå°±æ˜¯å‰é¢æ‰€æåˆ°çš„[å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«](#å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«)é—®é¢˜ã€‚ä¸å´æ©è¾¾è€å¸ˆå¤„ç†æ•°æ®çš„æ–¹å¼ä¸åŒï¼Œæˆ‘æ›´ä¹ æƒ¯å°†æ•°æ®æŒ‰è¡Œå‘é‡çš„æ–¹å¼å­˜å‚¨è€Œéåˆ—å‘é‡ï¼Œå³éœ€å°†`X,Y` åˆ†åˆ«è¿›è¡Œè½¬ç½®å¤„ç†ã€‚  

<CodeGroup>
<CodeGroupItem title="Logistic">

```python
# ä»£ç æŠ„è‡ªGitHub
# çœç•¥å¯¼å…¥åŒ…çš„è¿‡ç¨‹

np.random.seed(1)  # åˆå§‹åŒ–éšæœºæ•°

X, Y = load_planar_dataset()
X, Y = X.T, Y.T  # è¡Œåˆ—è½¬ç½®

# è®­ç»ƒé€»è¾‘å›å½’çš„åˆ†ç±»å™¨
clf = sklearn.linear_model.LogisticRegressionCV()
clf.fit(X, Y)

# ç»˜åˆ¶é€»è¾‘åˆ†ç±»çš„è¾¹ç•Œ
plot_decision_boundary2(lambda x: clf.predict(x), X, Y)
plt.title("Logistic Regression")

# æ‰“å°å‡†ç¡®åº¦
LR_predictions = clf.predict(X)
print ('Accuracy of logistic regression: %d ' 
  % float((np.dot(Y.T, LR_predictions) 
  + np.dot(1 - Y.T,1 - LR_predictions)) 
  / float(Y.T.size) * 100) 
  + '% ' + "(percentage of correctly labelled datapoints)")
```

</CodeGroupItem>
<CodeGroupItem title="Plot_Boundary">

```python{8-14}
# å´æ©è¾¾è€å¸ˆçš„ä¾‹é¢˜ä¸­é‡‡æ ·æ•°æ®æŒ‰åˆ—æ¨ªå‘åˆ†å¸ƒ
# è€Œä¸€èˆ¬æŒ‰è¡Œçºµå‘åˆ†å¸ƒæ›´å¥½ç†è§£ï¼Œä¸‹é¢æ˜¯æŒ‰è¡Œåˆ†å¸ƒæ—¶ç»˜åˆ¶è¾¹ç•Œçš„ä»£ç 
def plot_decision_boundary2(model, X, y):  # è¿™é‡Œçš„æ¨¡å‹å…¶å®æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨å‡½æ•°
    # è®¾ç½®ç»˜å›¾èŒƒå›´
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # x è½´
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # y è½´
    h = 0.01  # æ­¥é•¿
    # ç”Ÿæˆç»˜å›¾åŒºåŸŸç½‘æ ¼
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # é¢„æµ‹å€¼
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)    
    # ç»˜åˆ¶ç­‰é«˜çº¿å›¾
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)
```

</CodeGroupItem>
</CodeGroup>

è¿™é‡Œä¹‹æ‰€ä»¥æŠŠè¿™æ®µä»£ç å•ç‹¬æ‹¿å‡ºæ¥åˆ†æï¼Œæ˜¯å› ä¸ºä½œå›¾ä¹Ÿæ˜¯æ•°æ®åˆ†æä¸­éå¸¸é‡è¦çš„éƒ¨åˆ†ã€‚å­¦çŸ¥è¯†çš„åŒæ—¶è¿˜è¦æŒæ¡ä¸ä¹‹åŒ¹é…çš„æµ‹è¯•çš„æ–¹æ³•ï¼Œä¸ç„¶å­¦çš„å¯¹ä¸å¯¹éƒ½ä¸çŸ¥é“ã€‚æœ‰äº†ç›´è§‚çš„å›¾ç¤ºï¼Œå¯ä»¥è®©æˆ‘ä»¬æ›´å¿«åœ°åˆ†æå‡ºé—®é¢˜æ‰€åœ¨ï¼Œç£¨åˆ€ä¸è¯¯ç æŸ´åŠŸã€‚æœ€ç»ˆç”Ÿæˆçš„å›¾åƒå¦‚ä¸‹ï¼Œä¸€ç›®äº†ç„¶ï¼š  
![é€»è¾‘å›å½’](06_deep_learning/logistic-regression.svg)  

### å•ä¸ªéšè—å±‚  
å…·æœ‰å•ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼ˆä»¥ä¸¤æ¬¡é‡‡æ ·æ•°æ®ä¸ºä¾‹ï¼Œéšè—å±‚æœ‰ä¸‰ä¸ªç¥ç»å…ƒï¼‰ï¼š  
![å•ä¸ªéšè—å±‚](06_deep_learning/single-hidden-layer.svg)  

ä»£ç ä¸Šé¢ä¸é€»è¾‘å›å½’çš„å˜åŒ–ä¸å¤§ï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯åœ¨å±‚ä¸å±‚ä¹‹é—´å¯¼æ•°ä¼ é€’æ—¶çš„è®¡ç®—ï¼Œè§[çŸ©é˜µçš„å¾®åˆ†](#çŸ©é˜µçš„å¾®åˆ†)  

<CodeGroup>
<CodeGroupItem title="å•éšè—å±‚">

```python{5,26}
# çœç•¥æ•°æ®å¯¼å…¥çš„è¿‡ç¨‹
np.random.seed(2)

# åˆå§‹åŒ–å‚æ•°
dim = 40  # éšè—å±‚çš„ç¥ç»å…ƒä¸ªæ•°ï¼Œ
# ä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªæƒé‡çŸ©é˜µçš„åˆ—æ•°
W1 = np.random.randn(2, dim) * 0.01
b1 = np.zeros(shape=(1, dim))
# ç”±å›¾åƒå¯çŸ¥ï¼Œä¸Šä¸€å±‚æƒé‡çš„åˆ—æ•°ï¼Œç­‰äºä¸‹ä¸€å±‚æƒé‡çš„è¡Œæ•°
W2 = np.random.randn(dim, 1) * 0.01
b2 = np.zeros(shape=(1, 1))
learning_rate = 0.1  # å­¦ä¹ ç‡ï¼Œä¹Ÿæ˜¯æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿

for i in range(10000):
    # æ­£å‘è¿‡ç¨‹ï¼šéšè—å±‚
    Z1 = np.dot(X, W1)+b1
    A1 = np.tanh(Z1)    
    # æ­£å‘è¿‡ç¨‹ï¼šè¾“å‡ºå±‚
    Z2 = np.dot(A1, W2)+b2
    A2 = sigmoid(Z2)
    lost = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
    
    # åå‘è¿‡ç¨‹ï¼šè¾“å‡ºå±‚
    dZ2 = A2-Y
    dW2 = np.dot(A1.T, dZ2)/len(dZ2)
    dA2 = np.dot(dZ2, W2.T)  # å±‚é—´å¯¼æ•°ä¼ é€’
    db2 = dZ2.mean(axis=0)/len(dZ2)
    # åå‘è¿‡ç¨‹ï¼šéšè—å±‚
    dZ1 = np.multiply(dA2, 1 - np.power(A1, 2))
    dW1 = np.dot(X.T, dZ1)/len(dZ1)
    db1 = dZ1.mean(axis=0)/len(dZ1)

    # æ›´æ­£ç³»æ•°
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2


# æœ€ç»ˆçš„ç¥ç»ç½‘ç»œæ¨¡å‹
def model(X):
    Z1 = np.dot(X, W1)+b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(A1, W2)+b2
    A2 = sigmoid(Z2)
    return np.around(A2)

# è¾“å‡ºå‡†ç¡®ç‡
c = (1-np.sum(np.abs(Y-model(X)))/len(X))*100
# ç»˜åˆ¶è¾¹ç•Œå›¾
plot_decision_boundary2(lambda x: model(x), X, Y)
plt.title('Single Hidden Layer with %s nodes(%s%%)'%(dim,c))
```  

</CodeGroupItem>
<CodeGroupItem  title="åŒéšè—å±‚">

```python{4-10,16-19,21-24,26-29,39-40,46-50}
# ä»…æ ‡å‡ºä¸å•éšè—å±‚æ”¹åŠ¨çš„éƒ¨åˆ†
np.random.seed(2)

d1, d2 = 10, 10
W1 = np.random.randn(2, d1) * 0.01
b1 = np.zeros(shape=(1, d1))
W2 = np.random.randn(d1, d2) * 0.01
b2 = np.zeros(shape=(1, d2))
W3 = np.random.randn(d2, 1) * 0.01
b3 = np.zeros(shape=(1, 1))
learning_rate = 0.1

for i in range(40000):
    Z1 = np.dot(X, W1)+b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(A1, W2)+b2
    A2 = np.tanh(Z2)
    Z3 = np.dot(A2, W3)+b3
    A3 = sigmoid(Z3)
    # lost = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
    dZ3 = A3-Y
    dW3 = np.dot(A2.T, dZ3)/len(dZ3)
    dA3 = np.dot(dZ3, W3.T)
    db3 = dZ3.mean(axis=0)/len(dZ3)
    
    dZ2 = np.multiply(dA3, 1 - np.power(A2, 2))
    dW2 = np.dot(A1.T, dZ2)/len(dZ2)
    db2 = dZ2.mean(axis=0)/len(dZ2)
    dA2 = np.dot(dZ2, W2.T)

    dZ1 = np.multiply(dA2, 1 - np.power(A1, 2))
    dW1 = np.dot(X.T, dZ1)/len(dZ1)
    db1 = dZ1.mean(axis=0)/len(dZ1)

    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3


def model(X):
    Z1 = np.dot(X, W1)+b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(A1, W2)+b2
    A2 = np.tanh(Z2)
    Z3 = np.dot(A2, W3)+b3
    A3 = sigmoid(Z3)
    return np.around(A3)

pdb(lambda x: model(x), X, Y)
c = (1-np.sum(np.abs(Y-model(X)))/len(X))*100
plt.title('2 Hidden Layer with %s-%s nodes(%s%%)' % (d1,d2, c))
```

</CodeGroupItem>
</CodeGroup>


é€šè¿‡æµ‹è¯•ä¸åŒçš„ç¥ç»å…ƒæ•°é‡ï¼Œå¯ä»¥å¾—åˆ°ä¸åŒçš„è¾¹ç•Œå›¾ï¼Œå¦‚ä¸‹ï¼š  
![å•éšè—å±‚åŒ…å«ä¸‰ä¸ªç¥ç»å…ƒ](06_deep_learning/single-hidden-layer-3-nodes.svg)   

## æ„é€ è‡ªå·±çš„ç¥ç»ç½‘ç»œ  
æ ¹æ®ä¹‹å‰çš„å­¦ä¹ ç»éªŒï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å°†ç¥ç»ç½‘ç»œæ¨¡å‹åˆ’åˆ†ä¸º`3`ç§å±‚ï¼šè¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚ã€‚å…¶ä¸­ï¼š  
- è¾“å…¥å±‚ï¼šåªæ˜¯å°†é‡‡é›†åˆ°çš„æ•°æ®åŸå°ä¸åŠ¨åœ°è¾“å‡ºç»™éšè—å±‚  
- éšè—å±‚ï¼šè¿›è¡Œæ­£å‘è¿‡ç¨‹ä¸åå‘è¿‡ç¨‹è¿ç®—  
- è¾“å‡ºå±‚ï¼šè®¡ç®—è¯¯å·®å‡½æ•°ï¼ŒåŠå…¶å¯¼æ•°  

æ¯ä¸€å±‚çš„è¾“å‡ºå°±æ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚äºæ˜¯æˆ‘ä»¬å°±èƒ½æŠ½è±¡å‡ºä¸€ä¸ªé€šç”¨çš„â€œå±‚â€çš„æ¦‚å¿µçŸ©é˜µï¼š  

å±‚-å±æ€§|dim/ç»´åº¦|in/è¾“å…¥|prev/ä¸Šä¸€çº§|w/æƒé‡|b/å¸¸æ•°|actv/æ¿€æ´»å‡½æ•°|grad/å¯¼æ•°|l_r/å­¦ä¹ ç‡|loss/è¯¯å·®å‡½æ•°|out/è¾“å‡º  
:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:  
è¾“å…¥å±‚|âˆš|âˆš| | | | | | | |âˆš  
éšè—å±‚|âˆš|âˆš|âˆš|âˆš|âˆš|âˆš|âˆš|âˆš| |âˆš  
è¾“å‡ºå±‚|âˆš|âˆš|âˆš| | | |âˆš| |âˆš|âˆš  

<CodeGroup>
<CodeGroupItem title="Layer">

```python{45-47,60,73,89,94}
import numpy as np
###############################################################
# Layers                                                      #
#  - Base                                                     #
#    - InputLayer                                             #
#    - HiddenLayer                                            #
#    - OutputLayer                                            #
###############################################################


class Layer(object):
    def __init__(self):
        pass

    def forward(self):
        pass

    def backward(self, grad: np.array):
        pass


class InputLayer(Layer):  
    def __init__(self, dataIn: np.array):
        super().__init__()
        self.dim = dataIn.shape[1]
        self.dataIn = dataIn

    def setDataIn(self, dataIn: np.array):
        self.dataIn = dataIn

    def forward(self):
        return self.dataIn

    def backward(self, grad: np.array):
        return


class HiddenLayer(Layer):  # æ¯ä¸€å±‚éƒ½å¯ä»¥ç‹¬ç«‹è®¾ç½®å­¦ä¹ ç‡
    def __init__(self, prevLayer: Layer, dim: int, learning_rate=0.01):
        assert(isinstance(prevLayer, Layer))
        super().__init__()
        self.prev = prevLayer
        self.dim = dim
        self.learning_rate = learning_rate
        # ä¸Šä¸€èŠ‚ç‚¹çš„ç»´åº¦ä¸æœ¬èŠ‚ç‚¹çš„ç»´åº¦å…±åŒç¡®å®šå‚æ•°çŸ©é˜µçš„ç»´åº¦
        self.w = np.random.randn(prevLayer.dim, self.dim) * 0.01
        self.b = np.zeros(shape=(1, self.dim))
        self.actv = X()  # é»˜è®¤æ— çº¿æ€§åŒ–ç¯èŠ‚

    def setActivation(self, activation: Activation):
        self.actv = activation

    def regenWeight(self, method):
        self.w = method(self.prev.dim, self.dim)

    def regenB(self, method):
        self.w = method(self.dim)

    def forward(self):
        self.dataIn = self.prev.forward()  # é€’å½’è°ƒç”¨ä¸Šä¸€å±‚çš„æ­£å‘è¿‡ç¨‹
        self.z = np.dot(self.dataIn, self.w)+self.b
        self.a = self.actv(self.z)
        return self.a

    def backward(self, grad: np.array):
        self.dA = grad
        self.dZ = np.multiply(self.dA, self.actv.gradient(self.z))
        self.dW = np.dot(self.dataIn.T, self.dZ)/len(self.dZ)
        self.db = self.dZ.mean(axis=0)
        dDataIn = np.dot(self.dZ, self.w.T)
        self.w = self.w - self.learning_rate * self.dW
        self.b = self.b - self.learning_rate * self.db
        self.prev.backward(dDataIn)  # é€’å½’è°ƒç”¨ä¸Šä¸€å±‚çš„åå‘è¿‡ç¨‹
        return


class OutputLayer(Layer):
    def __init__(self, prevLayer: Layer, dataOut: np.array, loss=Sigmoid_Loss):
        assert(isinstance(prevLayer, Layer))
        super().__init__()
        self.prev = prevLayer
        self.dataOut = dataOut
        self.setLoss(loss)

    def setLoss(self, loss: Loss):  # è®¾ç½®è¯¯å·®å‡½æ•°
        self.loss = loss(self.dataOut)

    def forward(self):
        loss = self.loss(self.prev.forward())
        return loss

    def backward(self):
        dLoss = self.loss.gradient(self.prev.forward())
        self.prev.backward(dLoss)

    def predict(self):  # é¢„æµ‹å€¼ï¼Œå…¶å®å°±æ˜¯æœ€åä¸€ä¸ªéšè—å±‚çš„è¾“å‡º
        return self.prev.forward()
```

</CodeGroupItem>
<CodeGroupItem title="Activation">

```python{12-13,24,27}
import numpy as np
###############################################################
# Activations                                                 #
#  - Sigmoid                                                  #
#  - Tanh                                                     #
#  - ReLU                                                     #
#  - LeakyReLU                                                #
###############################################################

# æ¯ä¸€ä¸ªæ¿€æ´»å‡½æ•°éƒ½ä¼šæœ‰å…¶å¯¼å‡½æ•°çš„å®šä¹‰
class Activation(object):
    def __call__(self, z: np.array) -> np.array:
        pass

    def gradient(self, z: np.array) -> np.array:
        pass


class LeakyReLU(Activation):
    def __init__(self, alpha=0.01) -> None:
        self.alpha = alpha

    def __call__(self, z: np.array) -> np.array:
        return np.where(z > 0, z, self.alpha)

    def gradient(self, z: np.array) -> np.array:
        return np.where(z > 0, 1, self.alpha)


class ReLU(LeakyReLU):
    def __init__(self) -> None:
        super().__init__(alpha=0)


class Tanh(Activation):
    def __init__(self) -> None:
        pass

    def __call__(self, z: np.array) -> np.array:
        return np.tanh(z)

    def gradient(self, z: np.array) -> np.array:
        a = self.__call__(z)
        return 1 - np.power(a, 2)


class Sigmoid(Activation):
    def __init__(self) -> None:
        return

    def __call__(self, z: np.array) -> np.array:
        return 1/(1+np.exp(-z))

    def gradient(self, z: np.array) -> np.array:
        a = self.__call__(z)
        return a*(1-a)

class X(Activation):  # ä»€ä¹ˆéƒ½ä¸åšï¼Œæ— éçº¿æ€§åŒ–ç¯èŠ‚
    def __init__(self) -> None:
        return

    def __call__(self, z: np.array) -> np.array:
        return z

    def gradient(self, z: np.array) -> np.array:
        return 1
```

</CodeGroupItem>
<CodeGroupItem title="Loss">

```python
import numpy as np
###############################################################
# Loss                                                        #
#  - Sigmoid_Loss                                             #
#  - SoftMax(todo)                                            #
###############################################################


class Loss(object):
    def __init__(self, dataOut) -> None:
        self.dataOut = dataOut

    def __call__(self, a: np.array) -> np.array:
        pass

    def gradient(self, a: np.array) -> np.array:
        pass


class Sigmoid_Loss(Loss):
    def __init__(self, dataOut) -> None:
        super().__init__(dataOut)

    def __call__(self, a: np.array) -> np.array:
        y = self.dataOut
        return -(y*np.log(a)+(1-y)*np.log(1-a))

    def gradient(self, a: np.array) -> np.array:
        y = self.dataOut
        return (a-y)/a/(1-a)
```

</CodeGroupItem>
<CodeGroupItem title="Usage">

```python{10-16,18-21,23-25}
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

np.random.seed(1)  # åˆå§‹åŒ–éšæœºæ•°
X, Y = load_planar_dataset()
X, Y = X.T, Y.T  # è¡Œåˆ—è½¬ç½®

# æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹
layerIn = InputLayer(X)
layerHidden1 = HiddenLayer(layerIn, 4)
layerHidden1.setActivation(Tanh())
layerHidden2 = HiddenLayer(layerHidden1, 1)
layerHidden2.setActivation(Sigmoid())
layerOut = OutputLayer(layerHidden2, Y)

# å¾ªç¯æ­£å‘è¿‡ç¨‹ä¸åå‘è¿‡ç¨‹
for i in range(40000):
    layerOut.forward()
    layerOut.backward()

# æ›´æ¢æµ‹è¯•æ•°æ®é›†ï¼Œå¹¶è¾“å‡ºé¢„æµ‹å€¼ï¼ˆè¿™é‡Œç”¨çš„è¿˜æ˜¯è®­ç»ƒé›†ï¼‰
layerIn.setDataIn(X)
res = np.around(layerOut.predict())

# éªŒè¯æ­£ç¡®ç‡
c = 1-np.sum(np.abs(Y-res))/len(X)

# ç»˜å›¾ #
# è®¾ç½®ç»˜å›¾èŒƒå›´
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
h = 0.01  # æ­¥é•¿
# ç”Ÿæˆç»˜å›¾åŒºåŸŸç½‘æ ¼
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
# å°†ç½‘æ ¼åæ ‡è®¾ç½®ä¸ºç›®æ ‡æ•°æ®é›†ï¼Œå¹¶ç”Ÿæˆé¢„æµ‹å€¼
layerIn.setDataIn(np.c_[xx.ravel(), yy.ravel()])
Z = np.around(layerOut.predict())
Z = Z.reshape(xx.shape)
# ç»˜åˆ¶ç­‰é«˜çº¿å›¾
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
plt.ylabel('x2')
plt.xlabel('x1')
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Spectral)
plt.title('My NN model[accuracy=%s%%]' % ( c))
```

</CodeGroupItem>
<CodeGroupItem title="Initialization">

```python
###############################################################
# Initialization                                              #
#  - HeInitialization                                         #
###############################################################


class HeInitialization(object):
    def __init__(self, alpha: float) -> None:
        super().__init__()
        self.alpha = alpha

    def __call__(self, prevDim: int, dim: int):
        return np.random.randn(dim, prevDim).T * np.sqrt(2/prevDim)
```

</CodeGroupItem>
</CodeGroup>

è¿™ä¸€èŠ‚çš„ä»£ç ç¼ºå°‘æ³¨é‡Šï¼Œä½†ä»”ç»†é˜…è¯»å‘ç°å¹¶ä¸å¤æ‚ï¼Œä¹Ÿç®—æ˜¯ç»™è‡ªå·±çš„ä¸€ä»½`6.1ğŸ`å§~  

### å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«ï¼ˆäºŒï¼‰  
ä¾ç„¶æ˜¯é‡‡ç”¨[å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«](#å›¾ç‰‡ä¸­æ˜¯å¦æœ‰çŒ«)ä¸­çš„æ•°æ®é›†ï¼Œåªä¸è¿‡è¿™æ¬¡è¦æ­å»ºå±‚æ•°æ›´å¤šçš„ç½‘ç»œ  
```python
import numpy as np
from layer import InputLayer, HiddenLayer, OutputLayer, Sigmoid, LeakyReLU, ReLU, Tanh, Sigmoid_Loss
from lr_utils import load_dataset

train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()
trX = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1)/255
trY = train_set_y_orig.reshape(train_set_x_orig.shape[0], -1)
teX = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1)/255
teY = test_set_y_orig.reshape(test_set_x_orig.shape[0], -1)

np.random.seed(1)

lIn = InputLayer(trX)
lHi1 = HiddenLayer(lIn, 20, 0.0075)
lHi1.setActivation(ReLU())
lHi2 = HiddenLayer(lHi1, 7,  0.0075)
lHi2.setActivation(ReLU())
lHi3 = HiddenLayer(lHi2, 5,  0.0075)
lHi3.setActivation(ReLU())
lHi4 = HiddenLayer(lHi3, 1,  0.0075)
lHi4.setActivation(Sigmoid())
lOut = OutputLayer(lHi4, trY)

for i in range(2500):
    loss = lOut.forward()
    lOut.backward()
    if i % 100 == 0:
        print(i, loss.sum()/len(loss))

veriTrain = 1 - np.abs((np.around(lOut.predict())-trY)).sum()/len(trY)
print(veriTrain)

lIn.setDataIn(teX)
veriTest = 1-np.abs((np.around(lOut.predict())-teY)).sum()/len(teY)
print(veriTest)
```
ä¸Šé¢ä»£ç åœ¨åªæœ‰ä¸¤ä¸ªéšè—å±‚æ—¶å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç²¾åº¦åˆ†åˆ«ä¸º`1,0.72`ï¼Œç„¶è€Œåœ¨å¢åŠ éšè—å±‚æ•°é‡åï¼Œæ¨¡å‹çš„ç²¾åº¦å´å¹¶æœªå¦‚é¢„æœŸä¸€æ ·å¢åŠ ï¼Œä¸»è¦è¡¨ç°åœ¨éšç€å±‚æ•°çš„å¢åŠ ï¼Œæ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦å˜æ…¢äº†ã€‚é—®é¢˜å‡ºåœ¨æƒé‡åˆå§‹åŒ–ã€‚  

```diff
class HiddenLayer(Layer):  # æ¯ä¸€å±‚éƒ½å¯ä»¥ç‹¬ç«‹è®¾ç½®å­¦ä¹ ç‡
    def __init__(self, prevLayer: Layer, dim: int, learning_rate=0.01):
        #...
        # ä¸Šä¸€èŠ‚ç‚¹çš„ç»´åº¦ä¸æœ¬èŠ‚ç‚¹çš„ç»´åº¦å…±åŒç¡®å®šå‚æ•°çŸ©é˜µçš„ç»´åº¦
-        self.w = np.random.randn(prevLayer.dim, self.dim) * 0.01
+        self.w = np.random.randn(prevLayer.dim, self.dim) / np.sqrt(prevLayer.dim)
        # ...
```  
æƒé‡çš„å¤§å°ä¼šå½±å“æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ï¼šå¦‚æœæƒé‡è¿‡å¤§ï¼Œå°±ä¼šå¯¼è‡´æ¨¡å‹æ¢¯åº¦ä¸‹é™é€Ÿåº¦å˜æ…¢ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢æåˆ°çš„é—®é¢˜ï¼›å¦‚æœæƒé‡è®¾ç½®è¿‡å°ï¼Œåˆ™ä¼šå¯¼è‡´æ¨¡å‹è¿‡äºç®€å•ï¼Œå¤±å»æ·±åº¦å­¦ä¹ çš„æ„ä¹‰ã€‚æ‰€ä»¥ä¸Šæ–‡é‡‡ç”¨äº†`Xavier Initialization`ï¼š`éšæœºæ•°*1/sqrt(ä¸Šä¸€å±‚ç»´åº¦)`ã€‚è€Œå¯¹äº`ReLU`æ¿€æ´»å‡½æ•°ï¼Œæ›´æ¨è`He Initialization`ï¼š`éšæœºæ•°*sqrt(2/ä¸Šä¸€å±‚ç»´åº¦)`ï¼Œå¹¶ä¸”æ­¤æ–¹æ³•åœ¨å…¶ä»–æ¿€æ´»å‡½æ•°ä¹Ÿç›¸å½“å¥½ä½¿ã€‚é¡ºä¾¿æä¸€å¥ï¼Œæˆ‘ä»¬ç»™æ¯ä¸€å±‚é¢„ç•™äº†é‡ç½®åˆå§‹å€¼çš„æ¥å£ã€‚åªéœ€ä¿®æ”¹ç®€å•çš„ä»£ç å³å¯è®©æˆ‘ä»¬çš„æ¨¡å‹é¡ºç•…åœ°è·‘èµ·æ¥ï¼š

```python{5,8,11,14,17,24,27-31}
# ä»…è®°å½•ä¿®æ”¹çš„éƒ¨åˆ†
lIn = InputLayer(trX)
lHi1 = HiddenLayer(lIn, 20, 0.0075)
lHi1.setActivation(ReLU())
lHi1.regenWeight(HeInitialization(2))  # é‡æ–°åˆå§‹åŒ–æƒé‡
lHi2 = HiddenLayer(lHi1, 7,  0.0075)
lHi2.setActivation(ReLU())
lHi2.regenWeight(HeInitialization(2))  # é‡æ–°åˆå§‹åŒ–æƒé‡
lHi3 = HiddenLayer(lHi2, 5,  0.0075)
lHi3.setActivation(ReLU())
lHi3.regenWeight(HeInitialization(2))  # é‡æ–°åˆå§‹åŒ–æƒé‡
lHi4 = HiddenLayer(lHi3, 1,  0.0075)
lHi4.setActivation(Sigmoid())
lHi4.regenWeight(HeInitialization(1))  # é‡æ–°åˆå§‹åŒ–æƒé‡
lOut = OutputLayer(lHi4, trY)

costs=[]

for i in range(2500):
    loss = lOut.forward()
    lOut.backward()
    if i % 100 == 0:
        print(i, loss.sum()/len(loss))
        costs.append(loss.sum()/len(loss))

# ç»˜åˆ¶è¯¯å·®å˜åŒ–æ›²çº¿
plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.title("Learning rate =" + str(0.0075))
plt.show()
```

å¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š  
![è¯¯å·®ä¸‹é™å›¾](07_my_dl_structure/loss.svg)

ä¹‹æ‰€ä»¥æŠŠè¿™ä¸€èŠ‚å•ç‹¬åˆ—å‡ºæ¥ï¼Œæ˜¯å› ä¸ºè¿™ä¸€èŠ‚æ˜¯å¯¹å‰é¢æ‰€æœ‰ç¬”è®°çš„ä¸€ä¸ªç³»ç»ŸåŒ–æ€»ç»“ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥æ‹¿æ¥ç”¨çš„æˆæœã€‚æ›´å¤šå±‚çš„ç¥ç»ç½‘ç»œä¹Ÿæ˜¯ä»¥æ­¤ç±»æ¨ï¼Œè¿™ä¾¿æ˜¯æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†ã€‚è‡³äºä»¥åï¼ŒåŸºæœ¬æ˜¯åŸºäºæ­¤åŸç†çš„æ‰©å±•ä¸ä¿®è¡¥ã€‚  



