{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLY  \n",
    "\n",
    "> `PLY` 包括两部分：`lex.py` 和`yacc.py` 分别负责词法分析和语法分析工作。因为`python` 的执行效率一般不会高过`C++` 甚至`Javascript`，有一个问题，高效固然很重要，可是编译器的效率有必要非常高吗？能否只关注易用性而通过其他方案解决编译效率的问题？  \n",
    "\n",
    "`PLY` 不能在`ipython` 中运行！！！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为LLVM 的存在，开发者可以将编译器的前端和后端解耦、减少了项目的复杂度。而且前端也有着成熟的解决方案。所以编译器开发的重点工作就到了`AST` 到`LLVM` 的中间表示层`IR` 的翻译工作。而关于词法分析器、语法分析器、语义分析和解释器的原理，在[12Tall/lsbasi_cn](https://github.com/12Tall/lsbasi_cn) 翻译中也有比较详细的解释。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lex  \n",
    "词法分析器的工作就是将一系列的输入字符串转化为`token` 流，在`ply.lex` 中，主要通过正则表达式来定义`token` 的规则，然后由内部的逻辑机制去自动匹配这些规则。其中，词法分析有两个重要的原则：  \n",
    "-  最长匹配原则：例如`intI` 会被识别为符号`intI` 而不会被识别为类型`int` 和符号`I`；   \n",
    "-  最先声明原则：如果要识别`==`，则需要将其定义在`=` 规则之前（函数或变量声明的位置）；    \n",
    "-  如果一个`token` 同时符合多个规则，则会将其识别为最先声明的。如`break` 会被识别为保留字而不是符号。  \n",
    "\n",
    "### lex 自动初始化  \n",
    "通过[PLY 源码阅读之 Lex 篇](https://re-ra.xyz/PLY-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B9%8B-Lex-%E7%AF%87/) 可以看到`lex` 初始化的过程。主要是利用反射机制来通过模块中的全局变量和函数名来构建词法分析器。但是使用了太多的python 技巧会造成程序的可读性变差。\n",
    "```python\n",
    "import sys\n",
    "\n",
    "def get_caller_module_dict(levels):\n",
    "    f = sys._getframe(levels)\n",
    "    ldict = f.f_globals.copy()\n",
    "    if f.f_globals != f.f_locals:\n",
    "        ldict.update(f.f_locals) \n",
    "    return ldict\n",
    "\n",
    "def lex(module=None):\n",
    "    if module:\n",
    "        _items = [(k,getattr(module,k)) for k in dir(module)]\n",
    "        ldict = dict(_items)\n",
    "    else:\n",
    "        ldict = get_caller_module_dict(2)\n",
    "```  \n",
    "\n",
    "于是我们也可以将`token` 定义在独立的文件中。例如`lex(module='tokens.py')`。而我们的项目结构也就自然成为以下形式：  \n",
    "```txt  \n",
    "+- app.py  \n",
    "+- tokens.py\n",
    "```\n",
    "而`token` 的规则统一定义在`tokens.py`。  \n",
    "\n",
    "### 词法定义  \n",
    "以官方的示例代码为例：  \n",
    "- 所有的规则都以`t_` 开头  \n",
    "- 简单`token` 定义为变量，需要特殊处理的定义为函数`t_NUMBER(t)`，其中`t` 应有以下属性：    \n",
    "  - `type` 字符串，符号类型  \n",
    "  - `value` 符号值  \n",
    "  - `lineno` 行号、`lexpos` 列号，可能同时存在于`t` 和`t.lexer` 中\n",
    "```python    \n",
    "import ply.lex as lex  \n",
    "\n",
    "##### 规则定义，重要 #####\n",
    "\n",
    "# token 列表，必须的变量   \n",
    "tokens = (\n",
    "   'NUMBER',   \n",
    "   # ...\n",
    "   'RPAREN',\n",
    ")  \n",
    "\n",
    "# 简单的token 定义，变量的形式  \n",
    "t_RPAREN  = r'\\)'\n",
    "\n",
    "# 复杂的token 定义，包含token 值等动作代码\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'  # python 函数中的首行字符串或正则表达式会被赋值给\n",
    "    # func.__doc__ 词法分析器自动初始化时会用到这个属性  \n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "# 默认规则：新行\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "# 默认规则：忽略，空白和tab  \n",
    "t_ignore  = ' \\t'\n",
    "\n",
    "# 默认规则：非法字符  \n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "# 构建词法分析器   \n",
    "lexer = lex.lex()\n",
    "\n",
    "##### 测试 #####\n",
    "\n",
    "# 测试输入\n",
    "data = '''\n",
    "3 + 4 * 10\n",
    "  + -20 *2\n",
    "'''\n",
    "\n",
    "lexer.input(data)\n",
    "\n",
    "# 枚举token 并输出\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break      # 没有更多输入\n",
    "    print(tok)\n",
    "```\n",
    "\n",
    "### 保留字  \n",
    "对于保留字，有着自己的规则，应该避免使用上面的定义方式：  \n",
    "```python\n",
    "reserved = {\n",
    "   'if' : 'IF',\n",
    "   'then' : 'THEN',\n",
    "   'else' : 'ELSE',\n",
    "   'while' : 'WHILE',\n",
    "   ...\n",
    "}\n",
    "\n",
    "tokens = ['LPAREN','RPAREN',...,'ID'] + list(reserved.values())\n",
    "\n",
    "def t_ID(t):\n",
    "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "    t.type = reserved.get(t.value,'ID')    # 查找保留字表\n",
    "    # 或者查找符号表  \n",
    "    # t.value = (t.value, symbol_lookup(t.value))\n",
    "    return t\n",
    "```\n",
    "\n",
    "### 丢弃token  \n",
    "\n",
    "可以定义一个无返回值的token，或者通过`ignore_` 前缀来定义：  \n",
    "```python  \n",
    "# 第一种方式  \n",
    "def t_COMMENT(t):\n",
    "    r'\\#.*'\n",
    "    pass # 注释，无返回值  \n",
    "\n",
    "# 第二种方式  \n",
    "t_ignore_COMMENT = r'\\#.*'\n",
    "```   \n",
    "\n",
    "### 行列信息   \n",
    "```python  \n",
    "# 在换行时，将词法分析器的行号增加  \n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "# 计算列信息：用时再说  \n",
    "def find_column(input, token):\n",
    "    line_start = input.rfind('\\n', 0, token.lexpos) + 1\n",
    "    return (token.lexpos - line_start) + 1\n",
    "```  \n",
    "\n",
    "### 字面字符   \n",
    "字面字符的优先级始终是最高的，可以通过以下规则来定义：  \n",
    "```python\n",
    "# 定义  \n",
    "literals = [ '+','-','*','/' ]  \n",
    "# 或者  \n",
    "literals = \"+-*/\"  \n",
    "\n",
    "def t_lbrace(t):\n",
    "    r'\\{'\n",
    "    t.type = '{'      # Set token type to the expected literal\n",
    "    return t\n",
    "\n",
    "def t_rbrace(t):\n",
    "    r'\\}'\n",
    "    t.type = '}'      # Set token type to the expected literal\n",
    "    return t\n",
    "```  \n",
    "\n",
    "### EOF  \n",
    "```python \n",
    "# EOF handling rule\n",
    "def t_eof(t):\n",
    "    # Get more input (Example)\n",
    "    more = input('... ')\n",
    "    if more:\n",
    "        self.lexer.input(more)\n",
    "        return self.lexer.token()\n",
    "    return None\n",
    "```  \n",
    "\n",
    "### 装饰器  \n",
    "可以通过装饰器来修饰token 函数：  \n",
    "```python  \n",
    "identifier       = r'(' + nondigit + r'(' + digit + r'|' + nondigit + r')*)'\n",
    "\n",
    "@TOKEN(identifier)\n",
    "def t_ID(t):\n",
    "    ...\n",
    "```  \n",
    "\n",
    "### 内部状态  \n",
    "- `lexer.lexpos` 列号  \n",
    "- `lexer.lineno` 行号  \n",
    "- `lexer.lexdata` 当前文本  \n",
    "- `lexer.lexmatch`  \n",
    "\n",
    "更高级的用法暂不涉及。  \n",
    "\n",
    "## 词法分析器工程化  \n",
    "代码参考：[【Python】Ply 简介](https://www.leyeah.com/article/python-introduction-ply-674897)  \n",
    "\n",
    "```python\n",
    "import ply.lex as lex\n",
    "\n",
    "reserved = {  # 保留字表，仅用于查找用\n",
    "    'if': 'IF',\n",
    "    'then': 'THEN',\n",
    "    'else': 'ELSE',\n",
    "    'while': 'WHILE',\n",
    "}\n",
    "\n",
    "tokens = ['NUMBER', 'SELFMINUS', 'MINUS', 'ID'] + list(reserved.values())\n",
    "literals = ['+', '{', '}', '*', '/']  # 只能是单字符\n",
    "t_ignore = (\" \")\n",
    "\n",
    "# 优先级：\n",
    "# 1. 保留字 > tokens > literals\n",
    "# 2. 函数 > 变量\n",
    "# 3. 先定义 > 后定义\n",
    "def t_SELFMINUS(t):\n",
    "    r'\\-\\-'\n",
    "    t.type = \"SELFMINUS\"\n",
    "    return t\n",
    "\n",
    "def t_MINUS(t):\n",
    "    r'\\-'\n",
    "    t.type = \"MINUS\"\n",
    "    return t\n",
    "\n",
    "def t_NUMBER(t):\n",
    "    r'\\d+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "# 标识符：包括保留字、符号\n",
    "def t_ID(t):  # 匹配标识符\n",
    "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "    t.type = reserved.get(t.value, 'ID') # 查找保留字表\n",
    "    # 或者查找符号表（符号表应该在遍历抽象语法树时构建）\n",
    "    # t.value = (t.value, symbol_lookup(t.value))\n",
    "    return t\n",
    "\n",
    "def t_error(t: lex.LexToken) -> lex.LexToken:\n",
    "    print(f\"Illegal character '{t.value}' in {t.lineno}:{t.lexpos}\")\n",
    "    t.lexer.skip(1)\n",
    "    \n",
    "    \n",
    "data = ''' iF1 else---{}\"'''\n",
    "\n",
    "lexer = lex.lex()\n",
    "lexer.input(data)\n",
    "\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    if not tok:\n",
    "        break      # No more input\n",
    "    print(tok)\n",
    "\n",
    "# # 输出：\n",
    "# LexToken(ID,'iF1',1,1)\n",
    "# LexToken(ELSE,'else',1,5)\n",
    "# LexToken(SELFMINUS,'--',1,9)\n",
    "# LexToken(MINUS,'-',1,11)\n",
    "# LexToken({,'{',1,12)\n",
    "# LexToken(},'}',1,13)\n",
    "# Illegal character '\"' in 1:14\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
